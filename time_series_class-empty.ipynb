{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series specificities - Practical Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is following the progression of the Time Series specificities class. It provides practical illustrations in Python to understand the notions we have seen in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Author: Fabrice JIMENEZ\n",
    "    \n",
    "Link to course materials: https://github.com/jfabrice/ml-class-time-series\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Preliminary loading with Google Colab](#Preliminary-loading-with-Google-Colab)<br>\n",
    "[Imports and dataset presentation](#Imports-and-dataset-presentation)<br>\n",
    "\n",
    "[1- Formulate the question](#1--Formulate-the-question)<br>\n",
    "[2- Preprocessing](#2--Preprocessing)<br>\n",
    "[3- Transformation steps](#3--Transformation-steps)<br>\n",
    "[4- Feature Engineering](#4--Feature-Engineering)<br>\n",
    "[5- Regular approach](#5--Regular-approach)<br>\n",
    "[6- Method validation](#6--Method-validation)<br>\n",
    "\n",
    "[Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary loading with Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using this notebook with Google Colab, please execute first the following cells, to retrieve the GitHub repository content, set the working directory and install required dependencies. Otherwise, ignore these 3 cells and move to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/jfabrice/ml-class-time-series.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('ml-class-time-series')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and dataset presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![question](https://raw.githubusercontent.com/jfabrice/ml-class-time-series/master/data/question.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/dataset.csv')\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df.set_index('time', inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many cycles do we have? How many points per cycle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('cycle').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 15 cycles, with very different number of points per cycle.\n",
    "\n",
    "Let's visualize 1 parameter in time for each cycle to see how cycles are spread out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df['cycle'].unique():\n",
    "    df[df['cycle']==c]['p1'].plot(style='o', alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cycles seem to be separated in time by periods with no data. \n",
    "\n",
    "Let's see what our parameter looks like within a cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['cycle']==1]['p1'].plot(style='o', alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['cycle']==3]['p1'].plot(style='o', alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values seem to be continuous (close in time = close in value) which is logical if they are physical parameters.\n",
    "\n",
    "The parameter seems to behave quite differently in different cycles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Formulate the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "What is the task?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Associate cycles which are similar\" : this is a <b>clustering task</b>. Each cycle will be a sample and we will group similar samples with unsupervised clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Are some cycles really different from the rest?\" : this is an <b>outlier detection</b> task. We can probably use the result of the clustering to identify samples which are far away from all group centers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "What is the scale?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the task, we have no choice to consider the scale of the cycle as a unit. 1 sample will be composed of 4 time series (4 parameters) within a cycle.\n",
    "\n",
    "In addition, we saw that the number of points is very different in each cycle, so we will have to summarize each cycle into a set of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Are there missing values?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Are the time steps all equal?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timescale = pd.Series(df.index)\n",
    "timescale.diff().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time steps are not always equal to 1 second..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check an example of gap of 7 seconds, to see what strategy we could use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timescale[timescale.diff() == pd.Timedelta('00:00:07')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['p1']['2001-01-10 00:10:30':'2001-01-10 00:11:30'].plot(style='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that values still seem to be continuous. This kind of gap can be interpolated with mean value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that gaps of more than 1 day correspond to time periods between cycles. Let's confirm that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture the gaps of more than 1 day\n",
    "timescale[timescale.diff() >= pd.Timedelta(days=1)].reset_index()['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture the start time of each cycle\n",
    "df.reset_index().groupby('cycle').min().reset_index()['time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the timestamps corresponding to these gaps are matching with the first point of each cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Conclusion</b>: We can resample data with equal steps of 1 second within each cycle, with mean interpolation (oversampling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = []\n",
    "\n",
    "# For each cycle, create a small dataframe with interpolation\n",
    "for c in df['cycle'].unique():\n",
    "    tmp = df[df['cycle'] == c].copy()\n",
    "    tmp = tmp.resample('1S').mean().interpolate(method='linear')\n",
    "    \n",
    "    newdf.append(tmp)\n",
    "    \n",
    "# Concatenate the results to have the complete dataframe\n",
    "newdf = pd.concat(newdf, axis=0)\n",
    "df = newdf.copy()\n",
    "del newdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check we only have time steps of 1 second or 1 day (between cycles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timescale = pd.Series(df.index)\n",
    "timescale.diff().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df['cycle'].unique():\n",
    "    df[df['cycle']==c]['p1'].plot(style='o', alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['p1']['2001-01-10 00:10:30':'2001-01-10 00:11:30'].plot(style='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have clearly defined samples (windows) which contain time series regularly indexed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Transformation steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "1D: Can we exploit the structure of the data for each time series ?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at the parameter values closely, we have a lot of constant steps, making the overall shape piecewise constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['cycle'] == 1]['p1'][150:400].plot(style='o', alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, looking at the global shapes of signals, this kind of behavior will only add noise or pollution when we want to study the dynamics of the signal, for instance the frequencies, or the derivative. Let's filter our signals by applying a rolling mean, with a suitable window size making our signals smooth!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['cycle'] == 1]['p1'][150:400].rolling('10S').mean().plot(style='r-o', alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply it on all cycles and all parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = []\n",
    "\n",
    "# For each cycle, create a small dataframe with rolling mean\n",
    "for c in df['cycle'].unique():\n",
    "    tmp = df[df['cycle'] == c].copy()\n",
    "    tmp = tmp.rolling('10S').mean()\n",
    "    \n",
    "    newdf.append(tmp)\n",
    "    \n",
    "# Concatenate the results to have the complete dataframe\n",
    "newdf = pd.concat(newdf, axis=0)\n",
    "df = newdf.copy()\n",
    "del newdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said we want to study the dynamics of the system. Periodicity does not seem to be relevant due to the global shape of the signal across cycles. However, adding the derivative of each signal can bring information on the evolution of these signals. We keep the original signals also because the absolute quantities can also help discriminate behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivatives = []\n",
    "\n",
    "# For each cycle, create a small dataframe with derivative values of signals\n",
    "for c in df['cycle'].unique():\n",
    "    tmp = df[df['cycle'] == c].diff().drop('cycle',axis=1)\n",
    "    tmp.columns = ['p1_diff','p2_diff','p3_diff','p4_diff']\n",
    "    \n",
    "    derivatives.append(tmp)\n",
    "    \n",
    "# Concatenate the results to have the complete dataframe\n",
    "derivatives = pd.concat(derivatives, axis=0)\n",
    "df = pd.concat([df,derivatives], axis=1).dropna(axis=0)\n",
    "del derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "nD: Can we exploit the structure of the data for 2 or more time series at a time?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at correlations and pairplots to study pairwise relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df[['p1','p2','p3','p4']].sample(300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['p1','p2','p3','p4']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be some correlation between parameters but not high enough to be used. Maybe PCA can give us more information by gathering all parameters. We don't need to scale our dataset because it is already scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "resPCA = pca.fit_transform(df[['p1','p2','p3','p4']])\n",
    "df['PC1'] = resPCA[:,0]\n",
    "df['PC2'] = resPCA[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PC1 - PC2 projection for each cycle\n",
    "for c in df['cycle'].unique():\n",
    "    plt.plot(df[df['cycle'] == c]['PC1'], df[df['cycle'] == c]['PC2'], 'o', alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some behaviors seem to be observable on the PC1 - PC2 axes, for certain cycles but not all. It might be interesting to keep PC1 and PC2 as variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, we have our transformed time series in our dataset with derived information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "What features can now be used to summarize each cycle?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To describe a cycle we want to estimate:\n",
    "   - The distribution of parameter values: info contained in p1, p2, p3, p4\n",
    "   - The distribution of parameter combination values (pairs or more): info contained in PC1 and PC2\n",
    "   - The dynamics - evolution of parameter values: info contained in p1_diff, p2_diff, p3_diff, p4_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could go further, compute Fast Fourier Transform and use the coefficients, or derivatives of principal components in time, etc. But for the sake of the exercise, and considering it is a first step, we will only keep basic features on the enriched time series we already have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to be applied to the dataframe of each cycle\n",
    "def computeFeatures(tab):\n",
    "    res = pd.Series()\n",
    "    \n",
    "    # For each time series we compute the following features\n",
    "    for p in [c for c in tab.columns if c != 'cycle']:\n",
    "        # Mean\n",
    "        res[p+'_mean'] = tab[p].mean()\n",
    "        # 1st decile\n",
    "        res[p+'_d1'] = tab[p].quantile(0.1)\n",
    "        # 9th decile\n",
    "        res[p+'_d9'] = tab[p].quantile(0.9)\n",
    "        # Standard deviation\n",
    "        res[p+'_std'] = tab[p].std()\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the feature computation on each cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.groupby('cycle').apply(computeFeatures)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5- Regular approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Clustering task\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 15 cycles and 40 features. We are clearly doomed by the curse of dimensionality! Let's do a PCA to reduce the dimension, and we don't forget to scale our features, because they don't have the same units!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=5)\n",
    "resPCA = pca.fit_transform((features - features.mean())/features.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check we have a sufficient number of components to explain most of the variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can perform clustering on the PCA components, let's use a hierarchical clustering to see how many groups we should have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hac = hierarchy.linkage(resPCA, method='ward', metric='euclidean')\n",
    "dendrogram = hierarchy.dendrogram(hac, labels=features.index.map(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A choice of 4 clusters seems reasonable! Let's cut the dendrogram at 4 groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['cluster'] = hierarchy.cut_tree(hac,4).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the clusters in the PCA projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['blue','red','orange','cyan']\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(len(features)):\n",
    "    ax.scatter(resPCA[i,0], resPCA[i,1], c=colors[int(features.iloc[i]['cluster'])], alpha=0.7)\n",
    "    ax.text(x=resPCA[i,0]+0.1, y=resPCA[i,1]+0.1, s=str(int(features.index[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize one of the time series of each cycle in the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in range(4):\n",
    "    plt.figure()\n",
    "    plt.title('CLUSTER '+str(cluster+1)+' - '+colors[cluster].upper()+' - PARAMETER p1')\n",
    "    for c in features.index:\n",
    "        if features['cluster'][c] == cluster:\n",
    "            tmp = df[df['cycle'] == c]['p1'].copy()\n",
    "            tmp.index = tmp.index - tmp.index.min()\n",
    "            tmp.plot(style='o', alpha=0.1)\n",
    "    plt.ylim([-4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Outlier detection task\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our clustering task already revealed the outliers! Cycles 3 and 8 are clear outliers. A doubt remains on cycles 1 and 4, to be studied in validation phase..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6- Method validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate our model, we need either: \n",
    "- a validation or test set, containing the ground truth (the clusters we had to find, the real anomalies...) - <b>Strong validation strategy</b>\n",
    "- a human interpretation of the results (an expert eye who is able to tell if the clusters make sense, or if the anomalies detected are real ones...) - <b>Weak validation</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the frame of this class, we don't have any validation dataset, so we would rely only on a human interpretation by an expert. According to their interpretation, we can go back in the different steps of this analysis to change some elements (different resampling, smoothing, transformation steps, choice of features, method for clustering...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is why it is very important not to get lost in your choices! To be able to play with these choices, to adjust according to the results validation. An idea could be to keep track with a simple schema like this one:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pipeline](https://raw.githubusercontent.com/jfabrice/ml-class-time-series/master/data/pipeline.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always think first about the principles, the intuition, the qualitative aspect behind all the available functions you can find online.\n",
    "\n",
    "- Many people can chain very complex algorithms together and get results which might be relevant for a problem.\n",
    "- Only a few can make the right choices to quickly optimize the resolution of a problem and assess its feasability..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
